<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
    <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>USTC-TD-dataset</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="https://avatars.githubusercontent.com/u/116997363">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</head>

<body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
                <a class="navbar-item" href="https://ustc-ivclab.github.io/">
                    <span class="icon">
                        <i class="fas fa-home"></i>
                    </span>
                </a>

                <!-- <div class="navbar-item has-dropdown is-hoverable">
                <a class="navbar-link">
                    More Research
                </a>
                <div class="navbar-dropdown">
                    <a class="navbar-item" href="">
                    </a>
                </div>
            </div> -->
            </div>

        </div>
    </nav>

    <!--访问人数-->
    <div class='visitorCount'>
        <span id="busuanzi_container_site_pv">Vistors:<br><span id="busuanzi_value_site_pv"></span></span>
    </div>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">USTC-TD: A Test Dataset and Benchmark for Image and
                            Video Coding in 2020s</h1>
                        <div class="is-size-5 publication-authors">
                            <!-- Author -->
                            <span class="author-block">
                                <a href="https://github.com/ustc-milkbrother">Zhuoyuan Li</a>,</span>
                            <span class="author-block">
                                <a href="https://junqi98.github.io/">Junqi Liao</a>,</span>
                            <span class="author-block">
                                <a href="https://github.com/Austin4USTC">Chuanbo Tang</a>,</span>
                            <span class="author-block">
                                <a href="https://htzhang99.github.io/">Haotian Zhang</a>,</span>
                            <span class="author-block">
                                <a href="http://home.ustc.edu.cn/~lyq010303/">Yuqi Li</a>,</span>
                            <br>
                            <span class="author-block">
                                <a href="https://esakak.github.io/academic/">Yifan Bian</a>,</span>
                            <span class="author-block">
                                <a href="https://www.neuralcodec.come">Xihua Sheng</a>,</span>
                            <span class="author-block">
                                <a href="https://github.com/mrliyao">Yao Li</a>,</span>
                            <span class="author-block"> <a href="https://blog.csdn.net/weixin_43721070">Xinmin
                                    Feng</a>,</span>
                            <span class="author-block"> <a href="https://blog.csdn.net/weixin_43721070">Changsheng
                                    Gao</a>,</span>
                            <br>
                            <span class="author-block"> <a href="https://faculty.ustc.edu.cn/lil1/en">Li
                                    Li</a>,</span>
                            <span class="author-block"> <a href="https://faculty.ustc.edu.cn/dongeliu/">Dong
                                    Liu</a>,</span><!-- <i class="fas fa-envelope icon" style="font-size: 12px; color: #116eda"></i> -->
                            <span class="author-block"> <a
                                    href="https://scholar.google.com/citations?user=5bInRDEAAAAJ&hl=zh-CN">Feng
                                    Wu</a></span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">University of Science and Technology of China (USTC),</span><br>
                            <span class="author-block">MoE Key Laboratory of Brain-inspired Intelligent Perception and
                                Cognition,</span><br>
                            <span class="author-block">Intelligent Visual Data Coding Laboratory (iVC Lab)</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2409.08481"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/EsakaK/USTC-TD"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="far fa-images"></i>
                                        </span>
                                        <span>Data</span>
                                    </a>
                                    <!-- Code Link. -->
                                    <span class="link-block">
                                        <a href="https://github.com/EsakaK/USTC-TD/tree/main/code"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fab fa-github"></i>
                                            </span>
                                            <span>Code</span>
                                        </a>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- <div class="container is-max-desktop"> -->
    <!-- <center><img src="static/images/datasets/top.webp" border="0" width="100%"></center> -->
    <!-- <h2 class="subtitle has-text-centered"> -->
    <!-- <div align="center"> -->
    <!-- <p style="text-align:justify; text-justify:inter-ideograph;width:100%; text-align: center;">Figure 1. -->
    <!-- Examples of USTC Image Test -->
    <!-- Dataset. </p> -->
    <!-- </div> -->
    <!-- <br> -->
    <!-- </div> -->

    <!--Top Image-->
    <div class="container is-max-desktop">
        <center><img src="static/images/datasets/videos/2.gif" border="0" width="33%"><img
                src="static/images/datasets/videos/4.gif" border="0" width="33%"><img
                src="static/images/datasets/videos/10.gif" border="0" width="33%"></center>
        <center><img src="static/images/datasets/videos/1.gif" border="0" width="33%"><img
                src="static/images/datasets/videos/6.gif" border="0" width="33%"><img
                src="static/images/datasets/videos/9.gif" border="0" width="33%"></center>
        <!-- <h2 class="subtitle has-text-centered"> -->
        <div align="center">
            <p style="text-align:justify; text-justify:inter-ideograph;width:100%; text-align: center;">Figure 1.
                Examples of USTC Video Test
                Dataset. </p>
        </div>
        <br>
    </div>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Image/video coding has been a remarkable research
                            area for both academia and industry for many years. Testing
                            datasets, especially high-quality image/video datasets are desirable for the justified
                            evaluation of coding-related research,
                            practical applications, and standardization activities. We put
                            forward a test dataset namely USTC-TD, which has been
                            successfully adopted in the practical end-to-end image/video
                            coding challenge of the IEEE International Conference on Visual
                            Communications and lmage Processing (VCIP) in 2022 and 2023.
                            USTC-TD contains 40 images at 4K spatial resolution and 10
                            video sequences at 1080p spatial resolution, featuring various
                            content due to the diverse environmental factors (e.g. scene type,
                            texture, motion, view) and the designed imaging factors (e.g.
                            illumination, lens, shadow). We quantitatively evaluate USTCTD on different image/video
                            features (spatial, temporal, color,
                            lightness), and compare it with the previous image/video test
                            datasets, which verifies the wider coverage and more diversity of
                            the proposed dataset. We also evaluate both classic standardized
                            and recent learned image/video coding schemes on USTC-TD
                            with PSNR and MS-SSIM, and provide an extensive benchmark
                            for the evaluated schemes. Based on the characteristics and
                            specific design of the proposed test dataset, we analyze the
                            benchmark performance and shed light on the future research
                            and development of image/video coding. All the data are released
                            online: <a href="https://esakak.github.io/USTC-TD">https://esakak.github.io/USTC-TD</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">

            <div class="columns is-centered">
                <!-- Image Dataset -->
                <div class="column">
                    <div class="content">
                        <h2 class="title is-3">Image Test Dataset</h2>
                        <!--Image 2022-->
                        <p>
                            Our proposed dataset aims to cover various scenarios,
                            and try to collect and simulate the data in the real-world
                            coding transmission scenes, which makes the evaluation of
                            image coding schemes more closer to the actual application.
                        </p>
                        <p>
                            Considering the various content elements, we combine
                            different environmental conditions (scene type, texture, view, etc) and captured conditions
                            (resolution, illumination, lens, shadow, etc)
                            in the collection process.
                        </p>
                        <center><img src="static/images/datasets/2022.webp" border="0" width="100%"></center>
                        <!-- <h2 class="subtitle has-text-centered"> -->
                        <div align="center">
                            <p style="text-align:justify; text-justify:inter-ideograph;width:100%;text-align: center;">
                                Figure 2.
                                Illustration of the image dataset in USTC-TD 2022. </p>
                        </div>
                        <br>
                        <!--Image 2023-->
                        <p>Compared to USTD-TD 2022, USTC-TD 2023 considers more extreme elements in real-world
                            scenes.</p>
                        <center><img src="static/images/datasets/2023.webp" border="0" width="100%"></center>
                        <!-- <h2 class="subtitle has-text-centered"> -->
                        <div align="center">
                            <p style="text-align:justify; text-justify:inter-ideograph;width:100%;text-align: center;">
                                Figure 3.
                                Illustration of the image dataset in USTC-TD 2023. </p>
                        </div>
                        <br>
                    </div>
                </div>
                <!--/ Image Dataset. -->
            </div>

            <div class="columns is-centered">
                <!-- Video Dataset -->
                <div class="column">
                    <div class="content">
                        <h2 class="title is-3">Video Test Dataset</h2>
                        <!--Video-->
                        <p>
                            Based on the characteristics of previous video datasets,
                            our proposed dataset aims to cover more typical
                            characteristics of video content. Compared to the image data,
                            temporal-domain properties are unique to video, especially in
                            the diverse motion types with more environmental conditions
                            in natural videos. There are usually multiple moving objects
                            of arbitrary shapes and various motion types in video frames,
                            leading to complex motion fields, which challenge the video
                            coding schemes. Therefore, we simulate the video
                            data with various temporal correlation types, including different
                            kinds of motion types and lens motion.
                        </p>

                        <div class="gif">
                            <img src="static/images/datasets/videos/1.gif">
                            <img src="static/images/datasets/videos/2.gif">
                            <img src="static/images/datasets/videos/3.gif">
                            <img src="static/images/datasets/videos/4.gif">
                            <img src="static/images/datasets/videos/5.gif">
                        </div>
                        <div class="gif">
                            <img src="static/images/datasets/videos/6.gif">
                            <img src="static/images/datasets/videos/7.gif">
                            <img src="static/images/datasets/videos/8.gif">
                            <img src="static/images/datasets/videos/9.gif">
                            <img src="static/images/datasets/videos/10.gif">
                        </div>


                        <div align="center">
                            <p style="text-align:justify; text-justify:inter-ideograph;width:100%;text-align: center;">
                                Figure 4.
                                Illustration of the video dataset in USTC-TD 2022. </p>
                        </div>
                        <br>
                    </div>
                </div>
                <!--/ Video Dataset -->
            </div>

            <!-- Analysis. -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Dataset Details</h2>
                    <!-- Classification. -->
                    <h3 class="title is-4">Construction</h3>
                    <div class="content has-text-justified">
                        <p>
                            Based on the characteristics of previous image/video datasets, our proposed dataset aims to
                            cover
                            various scenarios,
                            and try to collect and simulate the data in the real-world
                            coding transmission scenes, which makes the evaluation of
                            image/video coding schemes more closer to the actual application.
                        </p>
                        <div align="center">
                            <b>TABLE 1. THE CONFIGURATION OF USTC-TD 2022 IMAGE DATASET.</b>
                            <center> <img src="static/images/illustration/image_cons1.png"></center>
                        </div>
                        <br>
                        <div align="center">
                            <b>TABLE 2. THE CONFIGURATION OF USTC-TD 2023 IMAGE DATASET.</b>
                            <center> <img src="static/images/illustration/image_cons2.png"></center>
                        </div>
                        <br>
                        <div align="center">
                            <b>TABLE 3. THE CONFIGURATION OF USTC-TD 2023 VIDEO DATASET.</b>
                            <center> <img src="static/images/illustration/video_cons.png"></center>
                        </div>
                    </div>

                    <!-- Distribution. -->
                    <h3 class="title is-4">Analysis</h3>
                    <div class="content has-text-justified">
                        <p>
                            To comprehensively
                            verify the wide coverage of our proposed dataset for various content elements and
                            qualitatively analyze the
                            superiority of USTC-TD, we evaluate the USTC-TD on different
                            image/video features and compare it with the previous
                            image/video common test datasets (image datasets: Kodak, CLIC, Tecnick, video datasets: HEVC
                            CTC, VVC CTC, MCL-JCV, UVG).
                            For analysis
                            of image/video features, we select the spatial information (SI), colorfulness (CF),
                            lightness information (LI),
                            and temporal information (TI) to characterize each dataset
                            along the dimensions of space, color, lightness, and temporal
                            correlation, which are commonly used to evaluate the quality
                            of dataset.
                        </p>
                        <center><img src="static/images/illustration/image_analysis1.png" border="0" width="70%"><br>
                        </center>
                        <div align="center">
                            <p style="text-align:justify; text-justify:inter-ideograph;width:100%;text-align: center;">
                                Figure 5.
                                The visualization of the evaluation of spatial information (SI) and
                                colorfulness (CF) features on different image test datasets. Scatter diagram
                                represents the SI versus CF, and corresponding convex hulls indicates the
                                coverage of different datasets. The histogram represents the number of images
                                under different SI scores.</p>
                            <br>
                        </div>
                        <center><img src="static/images/illustration/image_analysis2.png" border="0" width="70%">
                        </center>
                        <div align="center">
                            <p style="text-align:justify; text-justify:inter-ideograph;width:100%;text-align: center;">
                                Figure 6.
                                The visualization of the evaluation of lightness information (LI) and
                                CF features on different image test datasets. Scatter diagram represents the LI
                                versus CF, and corresponding convex hulls indicates the coverage of different <br>
                                datasets. The histogram represents the number of images under different LI
                                scores.</p>
                            <br>
                        </div>
                        <center><img src="static/images/illustration/video_analysis.png" border="0" width="70%"><br>
                        </center>
                        <div align="center">
                            <p style="text-align:justify; text-justify:inter-ideograph;width:100%;text-align: center;">
                                Figure 7.
                                The visualization of the evaluation of temporal information (TI) and
                                SI features on different video test datasets. Scatter diagram represents the TI
                                versus SI, and corresponding convex hulls indicates the coverage of different
                                datasets. The histogram represents the number of videos under different TI
                                scores.</p><br>
                        </div>
                        <br>
                    </div>
                </div>
            </div>

            <!-- Statistic. -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Dataset Evaluation</h2>
                    <p>
                        In this section, we establish the baselines and evaluate recent
                        advanced state-of-the-art learned image/video compression
                        algorithms, and standardization activities with different metrics (PSNR, MS-SSIM, et al.),
                        and comprehensively benchmark their performance on our
                        proposed datasets.
                    </p><br>
                    <!-- RD curver. -->
                    <h3 class="title is-4">Rate-distortion Curves</h3>
                    <div class="content has-text-justified">
                        <center><img src="static/images/datasets/image_rd.png" border="0" width="100%">
                        </center>
                        <div align="center">
                            <p style="text-align:justify; text-justify:inter-ideograph;width:100%;text-align: center;">
                                Figure 8. Overall rate-distortion (RD) curves of advanced image compression schemes on
                                PSNR and MS-SSIM metrics, the results are evaluated on USTC-TD
                                image dataset 2022 and 2023.</p>
                        </div>
                    </div>
                    <div class="content has-text-justified">
                        <center><img src="static/images/datasets/video_rd.png" border="0" width="100%">
                        </center>
                        <div align="center">
                            <p style="text-align:justify; text-justify:inter-ideograph;width:100%;text-align: center;">
                                Figure 9. Overall rate-distortion (RD) curves of advanced video compression schemes on
                                PSNR and MS-SSIM metrics, the results are evaluated on USTC-TD
                                video dataset 2023.</p>
                        </div>
                    </div>

                    <!-- RD table. -->
                    <h3 class="title is-4">BD-RATE for PSNR/MS-SSIM Metrics</h3>
                    <div class="content has-text-justified">
                        <div align="center">
                            <b>TABLE IV. BD-RATE (%) COMPARISON FOR PSNR. THE ANCHOR IS VTM.</b>
                            <center> <img src="static/images/datasets/video_psnr.png"></center>
                        </div>
                    </div>
                    <div class="content has-text-justified">
                        <div align="center">
                            <b>TABLE V. BD-RATE (%) COMPARISON FOR MS-SSIM. THE ANCHOR IS VTM.</b>
                            <center> <img src="static/images/datasets/video_msssim.png"></center>
                        </div>
                    </div>
                </div>
                <br />
                <!--/ Video. -->
            </div>
        </div>
        <!--/ Animation. -->


        <!-- Concurrent Work. -->
        <!-- <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Related Links</h2>
                    <div class="content has-text-justified">
                    </div>
                </div>
            </div> -->
        <!--/ Concurrent Work. -->

        </div>
    </section>

    <!--Bibtex-->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre>
@arxiv{USTC-TD,
    author    = {Zhuoyuan Li*, Junqi Liao*, Xihua Sheng, Haotian Zhang, Yuqi Li, Chuanbo Tang, Yifan Bian, Xinmin Feng, Yao Li, Changsheng Gao, Li Li, and Dong Liu},
    title     = {USTC-TD: USTC Test Dataset for Image and Video Coding in 2020s},
    booktitle = {arxiv},
    year      = {2024},
}
            </pre>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop">
            <h2 class="title is-3">Contributors</h2>
            <h3 class="title is-4">Supervisors</h3>
            <div class="person">
                <div class="round-1-4" style="text-align: center">
                    <div style="padding-left: 15%; padding-right: 15%">
                        <div class="round">
                            <a href="https://faculty.ustc.edu.cn/dongeliu" target="_blank">
                                <img src="static/images/authors/ld.jpg" alt="Dong Liu" class="roundimg">
                            </a>
                        </div>
                        <p style="line-height: 130%;">Dong Liu<br></p>
                    </div>
                </div>
                <div class="round-1-4" style="text-align: center">
                    <div style="padding-left: 15%; padding-right: 15%">
                        <div class="round">
                            <a href="https://faculty.ustc.edu.cn/lil1/en" target="_blank">
                                <img src="static/images/authors/ll.jpg" alt="Li Li" class="roundimg">
                            </a>
                        </div>
                        <p style="line-height: 130%;">Li Li<br></p>
                    </div>
                </div>
                <div class="round-1-4" style="text-align: center">
                    <div style="padding-left: 15%; padding-right: 15%">
                        <div class="round">
                            <a href="https://ustc-ivclab.github.io/people/2023/06/01/gao,-changsheng.html"
                                target="_blank">
                                <img src="static/images/authors/gcs.jpg" alt="Changsheng Gao" class="roundimg">
                            </a>
                        </div>
                        <p style="line-height: 130%;">Changsheng Gao<br></p>
                    </div>
                </div>
            </div>
            <br><br>
            <h3 class="title is-4">Students</h3>
            <div class="person">
                <div class="round-1-4" style="text-align: center">
                    <div style="padding-left: 15%; padding-right: 15%">
                        <div class="round">
                            <a href="https://github.com/ustc-milkbrother" target="_blank">
                                <img src="static/images/authors/lzy.jpg" alt="Zhuoyuan Li" class="roundimg">
                            </a>
                        </div>
                        <p style="line-height: 130%;">Zhuoyuan Li<br><span style="font-size: 80%">Ph.D.</span></p>
                    </div>
                </div>
                <div class="round-1-4" style="text-align: center">
                    <div style="padding-left: 15%; padding-right: 15%">
                        <div class="round">
                            <a href="https://junqi98.github.io/" target="_blank">
                                <img src="static/images/authors/ljq.jpg" alt="Junqi Liao" class="roundimg">
                            </a>
                        </div>
                        <p style="line-height: 130%;">Junqi Liao<br><span style="font-size: 80%">Ph.D.</span></p>
                    </div>
                </div>
                <div class="round-1-4" style="text-align: center">
                    <div style="padding-left: 15%; padding-right: 15%">
                        <div class="round">
                            <a href="https://github.com/Austin4USTC" target="_blank">
                                <img src="static/images/authors/tcb.jpg" alt="Chuanbo Tang" class="roundimg">
                            </a>
                        </div>
                        <p style="line-height: 130%;">Chuanbo Tang<br><span style="font-size: 80%">Ph.D.</span></p>
                    </div>
                </div>
                <div class="round-1-4" style="text-align: center">
                    <div style="padding-left: 15%; padding-right: 15%">
                        <div class="round">
                            <a href="https://htzhang99.github.io/" target="_blank">
                                <img src="static/images/authors/zht.jpg" alt="Haotian Zhang" class="roundimg">
                            </a>
                        </div>
                        <p style="line-height: 130%;">Haotian Zhang<br><span style="font-size: 80%">Ph.D.</span></p>
                    </div>
                </div>
                <div class="round-1-4" style="text-align: center">
                    <div style="padding-left: 15%; padding-right: 15%">
                        <div class="round">
                            <a href="https://github.com/lyq133" target="_blank">
                                <img src="static/images/authors/lyq.jpg" alt="Yuqi Li" class="roundimg">
                            </a>
                        </div>
                        <p style="line-height: 130%;">Yuqi Li<br><span style="font-size: 80%">M.S.</span></p>
                    </div>
                </div>
                <div class="round-1-4" style="text-align: center">
                    <div style="padding-left: 15%; padding-right: 15%">
                        <div class="round">
                            <a href="http://esakak.github.io/PersonalPage" target="_blank">
                                <img src="static/images/authors/byf.jpg" alt="Yifan Bian" class="roundimg">
                            </a>
                        </div>
                        <p style="line-height: 130%;">Yifan Bian<br><span style="font-size: 80%">Ph.D.</span></p>
                    </div>
                </div>
                <div class="round-1-4" style="text-align: center">
                    <div style="padding-left: 15%; padding-right: 15%">
                        <div class="round">
                            <a href="https://www.neuralcodec.com" target="_blank">
                                <img src="static/images/authors/sxh.jpg" alt="Xihua Sheng" class="roundimg">
                            </a>
                        </div>
                        <p style="line-height: 130%;">Xihua Sheng<br><span style="font-size: 80%">Ph.D.</span></p>
                    </div>
                </div>
                <div class="round-1-4" style="text-align: center">
                    <div style="padding-left: 15%; padding-right: 15%">
                        <div class="round">
                            <a href="https://github.com/mrliyao" target="_blank">
                                <img src="static/images/authors/ly.jpg" alt="Yao Li" class="roundimg">
                            </a>
                        </div>
                        <p style="line-height: 130%;">Yao Li<br><span style="font-size: 80%">Ph.D.</span></p>
                    </div>
                </div>
                <div class="round-1-4" style="text-align: center">
                    <div style="padding-left: 15%; padding-right: 15%">
                        <div class="round">
                            <a href="https://ustc-ivclab.github.io/people/2022/09/01/feng,-xinmin.html" target="_blank">
                                <img src="static/images/authors/fxm.jpg" alt="Xinmin Feng" class="roundimg">
                            </a>
                        </div>
                        <p style="line-height: 130%;">Xinmin Feng<br><span style="font-size: 80%">M.S.</span></p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!--Acknowledgement-->
    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3">Acknowledgement</h2>
            <p>
                We appreciate the utilization and support of some organizations, and also thanks to the supervisors and
                USTC’s
                volunteers featured in the contribution of this dataset.
            </p><br>
            <p>
                <strong>Actors</strong>: Cunhui Dong, Ziyi Zhuang, Feihong Mei, Qiaoxi
                Chen, Bojun Liu.
            </p><br>
            <p>
                <strong>Testers</strong>: Jialin Li, Xiongzhuang Liang.
            </p><br>
            <p>
                <strong>Organizations</strong>: IEEE International Conference on Visual
                Communications and lmage Processing (VCIP) 2022 and
                2023.
            </p><br>
            Thanks to the <a href="https://ieee-dataport.org/"> IEEE Dataport</a> , we have submitted the data to this
            open-sourced dataset website
            for the convenient access of the IEEE community's researchers.
        </div>
    </section>

    <!--Copyrihgt-->
    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3">Copyright</h2>
            <p>
                The released images and sequences are captured and processed by the University of Science and Technology
                of China
                (USTC). All intellectual property rights remain with USTC.
                If the users need our datasets for their works, please cite the datasets paper or this website.
            </p><br>
            <p>
                <span style="color: red;">The following uses are allowed for the contributed dataset:</span><br>
                &nbsp&nbsp&nbsp&nbsp 1. Data (images and videos) may be published in research
                papers, technical reports, and development events.<br>
                &nbsp&nbsp&nbsp&nbsp 2. Data (images and videos) may be utilized by standardization activities. (e.g.,
                ITU4, MPEG5, AVS6,
                VQEG7, et al.).
            </p><br>
            <p>
                <span style="color: red;">The following uses are NOT allowed for the contributed dataset:</span><br>
                &nbsp&nbsp&nbsp&nbsp 1. Do not publish snapshots in product brochures.<br>
                &nbsp&nbsp&nbsp&nbsp 2. Do not use video for marketing purposes.<br>
                &nbsp&nbsp&nbsp&nbsp 3. Redistribution is not permitted.<br>
                &nbsp&nbsp&nbsp&nbsp 4. Do not use it in television shows, commercials, or movies.<br>
            </p>
        </div>
    </section>

    <!--Contact-->
    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3">Contact</h2>
            <p>
                If you have any questions or advice on these datasets, please contact us: <br>
                <strong>Zhuoyuan Li</strong>: email-zhuoyuanli@mail.ustc.edu.cn, wechat-ustc_lizhuoyuan <br>
                <strong>Junqi Liao</strong>: email-liaojq@mail.ustc.edu.cn, wechat-liaojq98 <br>
            </p><br>
            <p>

                If you have any questions or advice on this website, please contact <a
                    href="https://esakak.github.io/PersonalPage">Yifan Bian</a>.
            </p>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This website is licensed under a <a rel="license"
                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                        <p>
                            Website source code based on the <a
                                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>